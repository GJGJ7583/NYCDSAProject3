fit.protein.average = hclust(d.pair, method = "average")
# Answer here
par(mfrow = c(1, 3))
plot(fit.protein.single, hang = -1, main = "Protein Dendrogram of Single Linkage")
plot(fit.protein.complete, hang = -1, main = "Protein Dendrogram of Complete Linkage")
plot(fit.protein.average, hang = -1, main = "Protein Dendrogram of Average Linkage")
############################################
############################################
#####[08] Cluster Analysis Lecture Code#####
############################################
############################################
###########################
#####Tools for K-Means#####
###########################
#Retrieving the numerical measures of the iris dataset.
iris.meas = iris[, -5]
summary(iris.meas)
sapply(iris.meas, sd)
#Standardizing the variables.
iris.scale = as.data.frame(scale(iris.meas))
summary(iris.scale)
sapply(iris.scale, sd)
#Visualizing the width measurements.
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "Scaled Iris Data")
#Conducting the K-Means algorithm on the whole dataset.
set.seed(0)
km.iris = kmeans(iris.scale, centers = 3)
#Inspecting the output of the kmeans() function.
km.iris
#Visualizing the results against the truth.
par(mfrow = c(1, 2))
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "Single K-Means Attempt", col = km.iris$cluster)
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "True Species", col = iris$Species)
#Plotting the cluster centers over the data.
par(mfrow = c(1, 1))
plot(iris.scale$Petal.Width, iris.scale$Sepal.Width,
xlab = "Petal Width", ylab = "Sepal Width",
main = "Single K-Means Attempt", col = km.iris$cluster)
points(km.iris$centers[, 4], km.iris$centers[, 2], pch = 16, col = "blue")
#A function to help determine the number of clusters when we do not have an
#idea ahead of time.
wssplot = function(data, nc = 15, seed = 0) {
wss = (nrow(data) - 1) * sum(apply(data, 2, var))
for (i in 2:nc) {
set.seed(seed)
wss[i] = sum(kmeans(data, centers = i, iter.max = 100, nstart = 100)$withinss)
}
plot(1:nc, wss, type = "b",
xlab = "Number of Clusters",
ylab = "Within-Cluster Variance",
main = "Scree Plot for the K-Means Procedure")
}
#Visualizing the scree plot for the scaled iris data; 3 seems like a plausible
#choice.
wssplot(iris.scale)
#It is important to note the non-determininstic nature of the K-Means algorithm.
#Using the Old Faithful dataset.
faithful.scale = scale(faithful)
summary(faithful.scale)
#Visualizing the scaled data.
par(mfrow = c(1, 1))
plot(faithful.scale)
#Determining the number of clusters.
wssplot(faithful.scale)
#Clearly, by both visual inspection and an analysis of the scree plot, a 2
#cluster solution is the most appropriate; however, let's see what happens if
#we search for a 3 cluster solution.
set.seed(0)
km.faithful1 = kmeans(faithful.scale, centers = 3) #Running the K-means procedure
km.faithful2 = kmeans(faithful.scale, centers = 3) #5 different times, but with
km.faithful3 = kmeans(faithful.scale, centers = 3) #only one convergence of the
km.faithful4 = kmeans(faithful.scale, centers = 3) #algorithm each time.
km.faithful5 = kmeans(faithful.scale, centers = 3)
#Running the algorithm 100 different times and recording the solution with the
#lowest total within-cluster variance.
set.seed(0)
km.faithfulsim = kmeans(faithful.scale, centers = 3, nstart = 100)
#Visually & numerically inspecting the results.
par(mfrow = c(2, 3))
plot(faithful, col = km.faithful1$cluster,
main = paste("Single K-Means Attempt #1\n WCV: ",
round(km.faithful1$tot.withinss, 4)))
plot(faithful, col = km.faithful2$cluster,
main = paste("Single K-Means Attempt #2\n WCV: ",
round(km.faithful2$tot.withinss, 4)))
plot(faithful, col = km.faithful3$cluster,
main = paste("Single K-Means Attempt #3\n WCV: ",
round(km.faithful3$tot.withinss, 4)))
plot(faithful, col = km.faithful4$cluster,
main = paste("Single K-Means Attempt #4\n WCV: ",
round(km.faithful4$tot.withinss, 4)))
plot(faithful, col = km.faithful5$cluster,
main = paste("Single K-Means Attempt #5\n WCV: ",
round(km.faithful5$tot.withinss, 4)))
plot(faithful, col = km.faithfulsim$cluster,
main = paste("Best K-Means Attempt out of 100\n WCV: ",
round(km.faithfulsim$tot.withinss, 4)))
###########################################
#####Tools for Hierarchical Clustering#####
###########################################
library(flexclust) #Loading the flexclust library.
data(nutrient) #Loading the nutrient data.
help(nutrient) #Inspecting the data set; nutrients in meat, fish, and fowel.
nutrient
#Notice that the nutrient columns are in different measurements: calories,
#grams, and milligrams.
summary(nutrient)
sapply(nutrient, sd)
#We should scale the data.
nutrient.scaled = as.data.frame(scale(nutrient))
summary(nutrient.scaled)
sapply(nutrient.scaled, sd)
#We need to calcualte the pairwise distances between observations.
d = dist(nutrient.scaled)
#Using the hclust() function, we define the linkage manner by which we will
#cluster our data.
fit.single = hclust(d, method = "single")
fit.complete = hclust(d, method = "complete")
fit.average = hclust(d, method = "average")
#Creating various dendrograms.
par(mfrow = c(1, 3))
plot(fit.single, hang = -1, main = "Dendrogram of Single Linkage")
plot(fit.complete, hang = -1, main = "Dendrogram of Complete Linkage")
plot(fit.average, hang = -1, main = "Dendrogram of Average Linkage")
#Cut the dendrogram into groups of data.
clusters.average = cutree(fit.average, k = 5)
clusters.average
#Cut the dendrogram into groups of data.
clusters.average = cutree(fit.average, k = 2)
clusters.average
plot(fit.average, hang = -1, main = "Dendrogram of Average Linkage\n5 Clusters")
# Answer here
group.two.complete <- cutree(fit.protein.complete, k = 2)
plot(group.two.complete, hang = -1, main = "Protein Dendrogram of Complete Linkage Two Groups")
aggregate(protein.scaled, by = list(cluster = group.two.complete), median)
plot(group.two.complete, hang = -1, main = "Protein Dendrogram of Complete Linkage Two Groups")
# Answer here
group.two.complete <- cutree(fit.protein.complete, k = 2)
plot(group.two.complete, hang = -1, main = "Protein Dendrogram of Complete Linkage Two Groups")
aggregate(protein.scaled, by = list(cluster = group.two.complete), median)
aggregate(nutrient.scaled, by = list(cluster = clusters.average), median)
# Answer here
group.two.complete <- cutree(fit.protein.complete, k = 5)
plot(group.two.complete, hang = -1, main = "Protein Dendrogram of Complete Linkage Two Groups")
aggregate(protein.scaled, by = list(cluster = group.two.complete), median)
# Answer here
protein = read.table("https://s3.amazonaws.com/nycdsabt01/Protein.txt", sep = "\t", header = TRUE)
protein.scaled = as.data.frame(scale(protein[, -1]))
rownames(protein.scaled) = protein$Country
# Answer here
library(psych)
fa.parallel(protein.scaled, fa = "pc", n.iter = 100)
# Answer here
set.seed(0)
km.protein.scaled1 = kmeans(protein.scaled, centers = 3)
km.protein.scaled2 = kmeans(protein.scaled, centers = 3)
km.protein.scaled3 = kmeans(protein.scaled, centers = 3)
km.protein.scaled4 = kmeans(protein.scaled, centers = 3)
km.protein.scaled5 = kmeans(protein.scaled, centers = 3)
# Answer here
km.protein.scaled100 = kmeans(protein.scaled, centers = 3, nstart = 100)
# Answer here
km.protein.scaled100 = kmeans(protein.scaled, centers = 3, nstart = 100)
# Answer here
par(mfrow = c(2, 3))
plot(protein.scaled$Cereals, protein.scaled$RedMeat,
xlab = "Cereals", ylab = "RedMeat",
main = "Single K-Means Attempt 1", col = km.protein.scaled1$cluster)
plot(protein.scaled$Cereals, protein.scaled$RedMeat,
xlab = "Cereals", ylab = "RedMeat",
main = "Single K-Means Attempt  2", col = km.protein.scaled2$cluster)
plot(protein.scaled$Cereals, protein.scaled$RedMeat,
xlab = "Cereals", ylab = "RedMeat",
main = "Single K-Means Attempt  3", col = km.protein.scaled3$cluster)
plot(protein.scaled$Cereals, protein.scaled$RedMeat,
xlab = "Cereals", ylab = "RedMeat",
main = "Single K-Means Attempt 4", col = km.protein.scaled4$cluster)
plot(protein.scaled$Cereals, protein.scaled$RedMeat,
xlab = "Cereals", ylab = "RedMeat",
main = "Single K-Means Attempt  5", col = km.protein.scaled5$cluster)
plot(protein.scaled$Cereals, protein.scaled$RedMeat,
xlab = "Cereals", ylab = "RedMeat",
main = "Single K-Means Attempt 100 Times", col = km.protein.scaled100$cluster)
# Answer here
d.pair = dist(protein.scaled)
# Answer here
fit.protein.single = hclust(d.pair, method = "single")
fit.protein.complete = hclust(d.pair, method = "complete")
fit.protein.average = hclust(d.pair, method = "average")
# Answer here
par(mfrow = c(1, 3))
plot(fit.protein.single, hang = -1, main = "Protein Dendrogram of Single Linkage")
plot(fit.protein.complete, hang = -1, main = "Protein Dendrogram of Complete Linkage")
plot(fit.protein.average, hang = -1, main = "Protein Dendrogram of Average Linkage")
# Answer here
group.two.complete <- cutree(fit.protein.complete, k = 2)
plot(group.two.complete, hang = -1, main = "Protein Dendrogram of Complete Linkage Two Groups")
aggregate(protein.scaled, by = list(cluster = group.two.complete), median)
# Answer here
group.two.complete <- cutree(fit.protein.complete, k = 5)
plot(group.two.complete, hang = -1, main = "Protein Dendrogram of Complete Linkage Two Groups")
aggregate(protein.scaled, by = list(cluster = group.two.complete), median)
library(ISLR)
Hitters = na.omit(Hitters)
help(Hitters)
x = model.matrix(Salary ~ ., Hitters)[, -1]
y = Hitters$Salary
grid = 10^seq(5, -2, length = 100)
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models))
coef(ridge.models)
ridge.models$lambda[80]
coef(ridge.models)[, 80]
sqrt(sum(coef(ridge.models)[-1, 80]^2))
ridge.models$lambda[15]
coef(ridge.models)[, 15]
sqrt(sum(coef(ridge.models)[-1, 15]^2))
ridge.models$lambda[1]
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
predict(ridge.models, s = 50, type = "coefficients")
set.seed(0)
train = sample(1:nrow(x), 7*nrow(x)/10)
test = (-train)
y.test = y[test]
length(train)/nrow(x)
length(y.test)/nrow(x)
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
mean((ridge.lambda5 - y.test)^2)
ridge.largelambda = predict(ridge.models.train, s = 1e10, newx = x[test, ])
mean((ridge.largelambda - y.test)^2)
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
log(bestlambda.ridge)
ridge.bestlambdatrain = predict(ridge.models.train, s = bestlambda.ridge, newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)
ridge.bestlambdatrain = predict.cv.glmnet(cv.ridge.out, s ="lambda.min", newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)
library(caret)
tune.grid = expand.grid(lambda = grid, alpha=c(0))
ridge.caret = train(x[train, ], y[train],
method = 'glmnet',
trControl = train_control, tuneGrid = tune.grid)
train_control = trainControl(method = 'cv', number=10)
ridge.caret = train(x[train, ], y[train],
method = 'glmnet',
trControl = train_control, tuneGrid = tune.grid)
plot(ridge.caret, xTrans=log)
pred = predict.train(ridge.caret, newdata = x[test,])
mean((pred - y[test])^2)
predict(ridge.caret$finalModel, newdata = x[test,])
lasso.models = glmnet(x, y, alpha = 1, lambda = grid)
dim(coef(lasso.models))
lasso.models$lambda[80]
coef(lasso.models)[, 80]
sum(abs(coef(lasso.models)[-1, 80]))
lasso.models$lambda[15]
coef(lasso.models)[, 15]
sum(abs(coef(lasso.models)[-1, 15])) #L1 norm is essentially 0.
plot(lasso.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
predict(lasso.models, s = 50, type = "coefficients")
lasso.models.train = glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
lasso.lambda5 = predict(lasso.models.train, s = 5, newx = x[test, ])
mean((lasso.lambda5 - y.test)^2)
set.seed(0)
cv.lasso.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 1, nfolds = 10)
plot(cv.lasso.out, main = "Lasso Regression\n")
bestlambda.lasso = cv.lasso.out$lambda.min
bestlambda.lasso
log(bestlambda.lasso)
lasso.bestlambdatrain = predict(lasso.models.train, s = bestlambda.lasso, newx = x[test, ])
mean((lasso.bestlambdatrain - y.test)^2)
library(tree)
library(ISLR)
attach(Carseats)
colnames(Carseats)
#Looking at the variable of interest, Sales.
hist(Sales)
summary(Sales)
#Creating a binary categorical variable High based on the continuous Sales
#variable and adding it to the original data frame.
High = ifelse(Sales <= 8, "No", "Yes")
colnames(Carseats)
Carseats = data.frame(Carseats, High, stringsAsFactors = T)
colnames(Carseats)
#Fit a tree to the data; note that we are excluding Sales from the formula.
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
#Plotting the classification tree.
plot(tree.carseats)
text(tree.carseats, pretty = 0) #Yields category names instead of dummy variables.
#Detailed information for the splits of the classification tree.
tree.carseats
#Splitting the data into training and test sets by an 70% - 30% split.
set.seed(2)
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.
Carseats.test = Carseats[-train, ] #Test dataset.
High.test = High[-train] #Test response.
#Ftting and visualizing a classification tree to the training data.
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
#Using the trained decision tree to classify the test data.
tree.pred = predict(tree.carseats, Carseats.test, type = "class")
tree.pred
#Assessing the accuracy of the overall tree by constructing a confusion matrix.
table(tree.pred, High.test)
#Performing cross-validation in order to decide how many splits to prune; using
#misclassification as the basis for pruning.
set.seed(0)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
#Inspecting the elements of the cv.tree() object.
names(cv.carseats)
cv.carseats
#Visually inspecting the results of the cross-validation by considering tree
#complexity.
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.carseats$k, cv.carseats$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
#Pruning the overall tree to have 4 terminal nodes; choose the best tree with
#4 terminal nodes based on cost complexity pruning.
par(mfrow = c(1, 1))
prune.carseats = prune.misclass(tree.carseats, best = 4)
plot(prune.carseats)
#Assessing the accuracy of the pruned tree with 4 terminal nodes by constructing
#a confusion matrix.
tree.pred = predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
##########################
#####Regression Trees#####
##########################
#Inspecting the housing values in the suburbs of Boston.
library(MASS)
#Creating a training set on 70% of the data.
set.seed(0)
train = sample(1:nrow(Boston), 7*nrow(Boston)/10)
colnames(Boston)
train = sample(1:nrow(Boston), 7*nrow(Boston)/10)
#Training the tree to predict the median value of owner-occupied homes (in $1k).
tree.boston = tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
#Visually inspecting the regression tree.
plot(tree.boston)
text(tree.boston, pretty = 0)
#Performing cross-validation.
set.seed(0)
cv.boston = cv.tree(tree.boston)
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
#Pruning the tree to have 4 terminal nodes.
prune.boston = prune.tree(tree.boston, best = 6)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
#Calculating and assessing the MSE of the test data on the overall tree.
yhat = predict(tree.boston, newdata = Boston[-train, ])
yhat
boston.test = Boston[-train, "medv"]
boston.test
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
library(tree)
##########################
#####Regression Trees#####
##########################
#Inspecting the housing values in the suburbs of Boston.
library(MASS)
colnames(Boston)
set.seed(0)
train = sample(1:nrow(Boston), 7*nrow(Boston)/10)
#Training the tree to predict the median value of owner-occupied homes (in $1k).
tree.boston = tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
#Visually inspecting the regression tree.
plot(tree.boston)
text(tree.boston, pretty = 0)
#Performing cross-validation.
set.seed(0)
cv.boston = cv.tree(tree.boston)
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
#Pruning the tree to have 4 terminal nodes.
prune.boston = prune.tree(tree.boston, best = 6)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
#Calculating and assessing the MSE of the test data on the overall tree.
yhat = predict(tree.boston, newdata = Boston[-train, ])
yhat
boston.test = Boston[-train, "medv"]
boston.test
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.boston, newdata = Boston[-train, ])
yhat
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
##################################
#####Bagging & Random Forests#####
##################################
library(randomForest)
#Fitting an initial random forest to the training subset.
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
rf.boston
#Varying the number of variables used at each step of the random forest procedure.
set.seed(0)
oob.err = numeric(13)
for (mtry in 1:13) {
fit = randomForest(medv ~ ., data = Boston[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
print(oob.err[mtry])
cat("We're performing iteration", mtry, "\n")
}
#Visualizing the OOB error.
plot(1:13, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
# The smallest err
min(oob.err)
# The smallest err occurs when mtry=6
which.min(oob.err)
#Can visualize a variable importance plot.
importance(rf.boston)
varImpPlot(rf.boston)
##################
#####Boosting#####
##################
library(gbm)
#Fitting 10,000 trees with a depth of 4.
set.seed(0)
boost.boston = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4)
#Inspecting the relative influence.
par(mfrow = c(1, 1))
summary(boost.boston)
#Letâs make a prediction on the test set. With boosting, the number of trees is
#a tuning parameter; having too many can cause overfitting. In general, we should
#use cross validation to select the number of trees. Instead, we will compute the
#test error as a function of the number of trees and make a plot for illustrative
#purposes.
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
#Produces 100 different predictions for each of the 152 observations in our
#test set.
dim(predmat)
#Calculating the boosted errors.
par(mfrow = c(1, 1))
berr = with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
#Include the best OOB error from the random forest.
abline(h = min(berr), col = "red")
# The smallest error:
min(berr)
validation <- function(d, s){
set.seed(0)
boost.boston2 = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = d,
shrinkage = s)
predmat2 = predict(boost.boston2, newdata = Boston[-train, ], n.trees = n.trees)
berr2 = with(Boston[-train, ], apply((predmat2 - medv)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
print(min(berr2))
}
validation(4, 0.1) # 10.28107
validation(5, 0.1) # 10.81959
validation(5, 0.05) # 9.674672
setwd("C:/Users/19178/Desktop/NYCDSA/Course/G Goginashvili Project 3")
train = read.csv("train.csv", header = TRUE)
test.data = read.csv("test.csv", header = TRUE)
sample_sub = read.csv("sample_submission.csv", header = TRUE)
sapply(train, class)
unique(train$KitchenQual)
unique(train$BsmtQual)
unique(train$MSZoning)
